{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a746e01d",
   "metadata": {},
   "source": [
    "# **DATA TRAINER USING BiLSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b091402",
   "metadata": {},
   "source": [
    "## **x.1 Setup and Reproducibility**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc9f155",
   "metadata": {},
   "source": [
    "#### x.1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2da112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf20d57",
   "metadata": {},
   "source": [
    "#### x.1.2 Device and Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd26de37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84ee0d7",
   "metadata": {},
   "source": [
    "## **x.2 Dataset Configuration and Utilities**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c080a4",
   "metadata": {},
   "source": [
    "#### x.2.1 Dataset Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eaee981",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_folder = Path(\"input/\")\n",
    "pad_len = 83\n",
    "n_features = 118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75c8a0",
   "metadata": {},
   "source": [
    "#### x.2.2 Filename Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c74738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_label_from_filename(csv_path):\n",
    "    parts = csv_path.stem.split(\"_\")\n",
    "    return \"_\".join(parts[:-1]) if len(parts) > 1 else csv_path.stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29046e7",
   "metadata": {},
   "source": [
    "#### x.2.3 Padding/truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e86c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate_with_length(sequence_2d, target_len, n_features):\n",
    "    T, F = sequence_2d.shape\n",
    "    length = min(T, target_len)\n",
    "\n",
    "    X = np.zeros((target_len, F), dtype=np.float32)\n",
    "    X[:length] = sequence_2d[:length].astype(np.float32)\n",
    "\n",
    "    return X, length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27ce768",
   "metadata": {},
   "source": [
    "## **x.3 Dataset Indexing and DataLoaders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14356fc2",
   "metadata": {},
   "source": [
    "#### x.3.1 Build file index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e916ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_file_index(csv_folder: Path):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - file_paths: list[Path]\n",
    "      - labels: list[str]\n",
    "      - label_to_id: dict[str, int]\n",
    "      - id_to_label: dict[int, str]\n",
    "    \"\"\"\n",
    "    file_paths = sorted(csv_folder.glob(\"*.csv\"))\n",
    "    if len(file_paths) == 0:\n",
    "        raise FileNotFoundError(f\"No CSV files found in: {csv_folder}\")\n",
    "\n",
    "    labels = [parse_label_from_filename(p) for p in file_paths]\n",
    "\n",
    "    unique_labels = sorted(set(labels))\n",
    "    label_to_id = {lab: i for i, lab in enumerate(unique_labels)}\n",
    "    id_to_label = {i: lab for lab, i in label_to_id.items()}\n",
    "\n",
    "    return file_paths, labels, label_to_id, id_to_label\n",
    "\n",
    "\n",
    "file_paths, labels, label_to_id, id_to_label = build_file_index(csv_folder)\n",
    "\n",
    "print(\"Total sequences:\", len(file_paths))\n",
    "print(\"Total classes  :\", len(label_to_id))\n",
    "print(\"Example labels :\", list(label_to_id.keys())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e80a79",
   "metadata": {},
   "source": [
    "#### x.3.2 Train/Val/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a1b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(file_paths, labels, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Returns dict with keys: train, val, test\n",
    "    Each value: list of indices\n",
    "    \"\"\"\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-9\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    label_to_indices = defaultdict(list)\n",
    "    for i, lab in enumerate(labels):\n",
    "        label_to_indices[lab].append(i)\n",
    "\n",
    "    train_idx, val_idx, test_idx = [], [], []\n",
    "\n",
    "    for lab, idxs in label_to_indices.items():\n",
    "        idxs = np.array(idxs)\n",
    "        rng.shuffle(idxs)\n",
    "\n",
    "        n = len(idxs)\n",
    "        n_train = int(round(n * train_ratio))\n",
    "        n_val = int(round(n * val_ratio))\n",
    "        # ensure total = n\n",
    "        n_test = n - n_train - n_val\n",
    "\n",
    "        train_part = idxs[:n_train]\n",
    "        val_part = idxs[n_train:n_train + n_val]\n",
    "        test_part = idxs[n_train + n_val:]\n",
    "\n",
    "        train_idx.extend(train_part.tolist())\n",
    "        val_idx.extend(val_part.tolist())\n",
    "        test_idx.extend(test_part.tolist())\n",
    "\n",
    "    # shuffle within splits (optional)\n",
    "    rng.shuffle(train_idx)\n",
    "    rng.shuffle(val_idx)\n",
    "    rng.shuffle(test_idx)\n",
    "\n",
    "    return {\"train\": train_idx, \"val\": val_idx, \"test\": test_idx}\n",
    "\n",
    "\n",
    "splits = stratified_split(file_paths, labels, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1)\n",
    "\n",
    "print(\"Train:\", len(splits[\"train\"]))\n",
    "print(\"Val  :\", len(splits[\"val\"]))\n",
    "print(\"Test :\", len(splits[\"test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5bbc65",
   "metadata": {},
   "source": [
    "#### x.3.3 PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3f11cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignCSVDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Lazy CSV dataset:\n",
    "      - loads one CSV when __getitem__ is called\n",
    "      - pads/truncates to pad_len\n",
    "      - returns (X, y, length, filename)\n",
    "    \"\"\"\n",
    "    def __init__(self, file_paths, labels, label_to_id, pad_len, n_features=118):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.label_to_id = label_to_id\n",
    "        self.pad_len = pad_len\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        csv_path = self.file_paths[idx]\n",
    "        label_str = self.labels[idx]\n",
    "        y = self.label_to_id[label_str]\n",
    "\n",
    "        # Load CSV (T, 118)\n",
    "        df = pd.read_csv(csv_path)\n",
    "        seq = df.values  # numpy array\n",
    "\n",
    "        # Pad/truncate (pad_len, 118) and get original length\n",
    "        X_np, length = pad_or_truncate_with_length(seq, self.pad_len, self.n_features)\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        X = torch.from_numpy(X_np)                # float32, shape (T, F)\n",
    "        y = torch.tensor(y, dtype=torch.long)     # class id\n",
    "        length = torch.tensor(length, dtype=torch.long)\n",
    "\n",
    "        return X, y, length, csv_path.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fe3ea9",
   "metadata": {},
   "source": [
    "#### x.3.4 Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c928ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    X_list, y_list, length_list, fname_list = zip(*batch)\n",
    "\n",
    "    X_batch = torch.stack(X_list, dim=0)       # (B, T, F)\n",
    "    y_batch = torch.stack(y_list, dim=0)       # (B,)\n",
    "    lengths = torch.stack(length_list, dim=0)  # (B,)\n",
    "\n",
    "    return X_batch, y_batch, lengths, list(fname_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5737e4b1",
   "metadata": {},
   "source": [
    "#### x.3.5 DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5931dfcb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SIBI_LSTMTrainer312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
